{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2898acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym installation seems to be working!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "env.close()\n",
    "print(\"Gym installation seems to be working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8df2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e15e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVChargingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A toy Markov Decision Process for EV charging, using the newer Gym API\n",
    "    that returns (obs, reward, done, truncated, info) from step().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vehicles=3,\n",
    "        num_time_periods=5,\n",
    "        charging_rate=0.2,\n",
    "        discharging_rate=0.1,\n",
    "        electricity_price=None,\n",
    "        soc_min=0.0,\n",
    "        soc_max=1.0,\n",
    "        desired_soc=0.8,\n",
    "        penalty_deviation=1.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_vehicles = num_vehicles\n",
    "        self.num_time_periods = num_time_periods\n",
    "        self.charging_rate = charging_rate\n",
    "        self.discharging_rate = discharging_rate\n",
    "        self.soc_min = soc_min\n",
    "        self.soc_max = soc_max\n",
    "\n",
    "        # If electricity_price is not provided, use a simple placeholder array\n",
    "        if electricity_price is None:\n",
    "            electricity_price = np.ones(num_time_periods) * 10.0\n",
    "        self.electricity_price = np.array(electricity_price)\n",
    "\n",
    "        self.desired_soc = desired_soc\n",
    "        self.penalty_deviation = penalty_deviation\n",
    "\n",
    "        # Current time step\n",
    "        self.current_time = 0\n",
    "\n",
    "        # Define the observation space:\n",
    "        # - We'll represent the state-of-charge as a float in [soc_min, soc_max]\n",
    "        #   for each vehicle. So shape is (num_vehicles,).\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.float32(soc_min),\n",
    "            high=np.float32(soc_max),\n",
    "            shape=(self.num_vehicles,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Define the action space:\n",
    "        # - For each vehicle, we choose {0,1} => not charging / charging.\n",
    "        # - So it's a MultiBinary space of length num_vehicles.\n",
    "        self.action_space = spaces.MultiBinary(self.num_vehicles)\n",
    "\n",
    "        # Internal state: array of shape (num_vehicles,) for SOC\n",
    "        self.soc = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets the environment at the start of an episode.\n",
    "        By Gym convention, returns (observation, info).\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.current_time = 0\n",
    "\n",
    "        # Initialize SOC in [0.5, 0.7], or set them to your desired initial conditions\n",
    "        self.soc = np.random.uniform(low=0.5, high=0.7, size=(self.num_vehicles,))\n",
    "        \n",
    "        # Return the initial observation and an empty info dict\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one step of the environment dynamics given the action.\n",
    "\n",
    "        Returns:\n",
    "          - obs: next observation\n",
    "          - reward\n",
    "          - done (bool): True if episode finished due to success/failure\n",
    "          - truncated (bool): True if episode ended due to timelimit or other truncation\n",
    "          - info: dict with additional info\n",
    "        \"\"\"\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "\n",
    "        # Compute reward = - (cost + penalty)\n",
    "        price_t = self.electricity_price[self.current_time]\n",
    "        num_charging = np.sum(action)\n",
    "        cost = price_t * num_charging  # simplistic cost\n",
    "\n",
    "        # Deviation penalty from desired SOC\n",
    "        deviation = np.abs(self.soc - self.desired_soc)\n",
    "        penalty = self.penalty_deviation * np.sum(deviation)\n",
    "\n",
    "        # Typically in RL we *maximize* reward, so we make cost/penalty negative:\n",
    "        reward = - (cost + penalty)\n",
    "\n",
    "        # State transition\n",
    "        for v in range(self.num_vehicles):\n",
    "            if action[v] == 1:\n",
    "                # charging\n",
    "                self.soc[v] += self.charging_rate\n",
    "            else:\n",
    "                # discharging\n",
    "                self.soc[v] -= self.discharging_rate\n",
    "\n",
    "            # clamp to [soc_min, soc_max]\n",
    "            self.soc[v] = np.clip(self.soc[v], self.soc_min, self.soc_max)\n",
    "\n",
    "        self.current_time += 1\n",
    "\n",
    "        # done = True if we reached the final time period\n",
    "        done = (self.current_time >= self.num_time_periods)\n",
    "        # We can optionally incorporate a time-limit or other logic to set truncated=True\n",
    "        truncated = False\n",
    "\n",
    "        info = {}\n",
    "        return self._get_obs(), reward, done, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation, which is the array of SOCs.\n",
    "        \"\"\"\n",
    "        return self.soc.astype(np.float32)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Optional: Provide a visualization or textual printout of the environment.\n",
    "        \"\"\"\n",
    "        print(f\"Time: {self.current_time}, SOC: {self.soc}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Optional cleanup when closing the environment.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5c6315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1, SOC: [0.54633299 0.58785279 0.76547381]\n",
      "Time: 2, SOC: [0.44633299 0.48785279 0.96547381]\n",
      "Time: 3, SOC: [0.64633299 0.68785279 0.86547381]\n",
      "Time: 4, SOC: [0.54633299 0.88785279 1.        ]\n",
      "Time: 5, SOC: [0.44633299 1.         0.9       ]\n",
      "Episode finished! Total reward: -80.52388342199956\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Example usage with the new Gym step signature:\n",
    "if __name__ == \"__main__\":\n",
    "    env = EVChargingEnv(\n",
    "        num_vehicles=3,\n",
    "        num_time_periods=5,\n",
    "        charging_rate=0.2,\n",
    "        discharging_rate=0.1,\n",
    "        electricity_price=[10, 12, 8, 9, 11],\n",
    "        desired_soc=0.8,\n",
    "        penalty_deviation=5.0\n",
    "    )\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    schedule_log = []\n",
    "\n",
    "    while not done:\n",
    "        # Random action (0 or 1 for each vehicle)\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # Combine done & truncated if you want a single termination condition\n",
    "        done = done or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "        # Store the scheduling decisions\n",
    "        schedule_log.append({\n",
    "            'time': env.current_time,\n",
    "            'action': action.copy(),\n",
    "            'SOC': obs.copy(),\n",
    "            'reward': reward\n",
    "        })\n",
    "        env.render()\n",
    "\n",
    "    print(\"Episode finished! Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "010f9459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'time': 1,\n",
       "  'action': array([0, 0, 1], dtype=int8),\n",
       "  'SOC': array([0.546333 , 0.5878528, 0.7654738], dtype=float32),\n",
       "  'reward': -12.501702044933191},\n",
       " {'time': 2,\n",
       "  'action': array([0, 0, 1], dtype=int8),\n",
       "  'SOC': array([0.446333  , 0.48785278, 0.96547383], dtype=float32),\n",
       "  'reward': -14.50170204493319},\n",
       " {'time': 3,\n",
       "  'action': array([1, 1, 0], dtype=int8),\n",
       "  'SOC': array([0.646333 , 0.6878528, 0.8654738], dtype=float32),\n",
       "  'reward': -20.156440148256856},\n",
       " {'time': 4,\n",
       "  'action': array([0, 1, 1], dtype=int8),\n",
       "  'SOC': array([0.546333 , 0.8878528, 1.       ], dtype=float32),\n",
       "  'reward': -19.656440148256856},\n",
       " {'time': 5,\n",
       "  'action': array([0, 1, 0], dtype=int8),\n",
       "  'SOC': array([0.446333, 1.      , 0.9     ], dtype=float32),\n",
       "  'reward': -13.707599035619461}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106edad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
